# 🚀 Transformers, Attention & Encoder-Decoder – Notes Vault

Welcome to my personal notes on core modern NLP architectures:  
**Transformers, Attention Mechanisms, and the Encoder-Decoder Framework.**

This repo is a collection of simplified concepts, diagrams, and explanations — built for quick reference, sharing, and deeper understanding.

---

## 📚 Table of Contents

### 🔁 Encoder-Decoder Architecture
- Overview of Seq2Seq models
- Use in Machine Translation
- Limitation of vanilla RNN encoder-decoder
- Transition to attention-based systems

### 🎯 Attention Mechanism
- Why attention? (Focus over entire input sequence)
- Types: Additive vs Scaled Dot Product
- Self-Attention Explained
- Attention Formula:  
  `Attention(Q, K, V) = softmax(QKᵀ / √d_k) * V`
- Visual intuition + weight matrices

### 🧠 Transformers
- "Attention Is All You Need" paper breakdown
- Architecture (Encoder Block, Decoder Block)
- Multi-head Self-Attention
- Positional Encoding
- Feedforward + Residual + Layer Norm
- GPT, BERT basics

---

## 🧠 Why This Exists

This is:
- A personal knowledge vault
- A crash-course reference for Transformers & attention
- My documentation of learning LLM foundations

---

## 🛠️ How to Use

- Browse folders for each concept
- Notes are written in Markdown for easy reading
- Ideal for fast refreshers or sharing with others

---

## 📌 Author

Maintained by [Harsh Gidwani – learning LLMs one layer at a time 🧠


