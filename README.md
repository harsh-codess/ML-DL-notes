# ğŸš€ Transformers, Attention & Encoder-Decoder â€“ Notes Vault

Welcome to my personal notes on core modern NLP architectures:  
**Transformers, Attention Mechanisms, and the Encoder-Decoder Framework.**

This repo is a collection of simplified concepts, diagrams, and explanations â€” built for quick reference, sharing, and deeper understanding.

---

## ğŸ“š Table of Contents

### ğŸ” Encoder-Decoder Architecture
- Overview of Seq2Seq models
- Use in Machine Translation
- Limitation of vanilla RNN encoder-decoder
- Transition to attention-based systems

### ğŸ¯ Attention Mechanism
- Why attention? (Focus over entire input sequence)
- Types: Additive vs Scaled Dot Product
- Self-Attention Explained
- Attention Formula:  
  `Attention(Q, K, V) = softmax(QKáµ€ / âˆšd_k) * V`
- Visual intuition + weight matrices

### ğŸ§  Transformers
- "Attention Is All You Need" paper breakdown
- Architecture (Encoder Block, Decoder Block)
- Multi-head Self-Attention
- Positional Encoding
- Feedforward + Residual + Layer Norm
- GPT, BERT basics

---

## ğŸ§  Why This Exists

This is:
- A personal knowledge vault
- A crash-course reference for Transformers & attention
- My documentation of learning LLM foundations

---

## ğŸ› ï¸ How to Use

- Browse folders for each concept
- Notes are written in Markdown for easy reading
- Ideal for fast refreshers or sharing with others

---

## ğŸ“Œ Author

Maintained by Harsh Gidwani â€“ learning LLMs one layer at a time ğŸ§ 


